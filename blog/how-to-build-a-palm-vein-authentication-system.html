<!DOCTYPE HTML>
<html>

<head>
    <title>How to Build a Palm Vein Authentication System</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Software Developer - Waterloo Software Engineering 2022" />
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
        integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="../css/index.css">
    <link rel="stylesheet" href="../css/blog.css">
</head>

<body>
    <ul class="nav justify-content-center">
		<li class="nav-item">
			<a class="nav-link active" href="/">Home</a>
		</li>
		<li class="nav-item">
			<a class="nav-link" href="/writing.html">Writing</a>
		</li>
		<li class="nav-item">
			<a class="nav-link" href="#contact">Contact</a>
		</li>
    </ul>

    <div class="header">
        <img class="img-header" src="../img/thumbnails/palm.png">
        <img class="profile-img" src="../img/headshot.jpg">
    </div>

    <div class="blog-title">
        How to Build a Palm Vein Authentication System
    </div>
    <div class="blog-date">June 7, 2020</div>

    <div class="blog-body">
        <div class="blog-section">
            <i>Palm vein authentication</i> is the process of using an individual's vein pattern to
            identify who they are.
            Despite not being very well known, it has the potential to become one of the best forms of biometric
            authentication.
            It's:
            <ul class="list">
                <li>
                    <strong>Contactless.</strong> A big plus for public settings like banks and hospitals.
                </li>
                <li>
                    <strong>Reliable.</strong> There are millions of identifiable points and regions in a vein pattern.
                </li>
                <li>
                    <strong>Difficult to impersonate.</strong> Even identical twins have different vein patterns.
                </li>
            </ul>
            However, one of the main drawbacks right now is the cost
            (sensors are currently selling for
            <a href="https://www.fulcrumbiometrics.com/fujitsu-palmsecure-vein-vascular-biometric-gu-p/102420.htm"
                target="_blank">$400+</a>).
            Here's how to make a simple but accurate one for under $90 using a Raspberry Pi, OpenCV, and TensorFlow.
        </div>

        <div class="subheading">
            What You'll Need
        </div>
        <div class="blog-section">
            <ul class="list">
                <li>
                    <a href="https://amzn.to/30kmUJg" target="_blank">
                        Raspberry Pi
                    </a>
                </li>
                <li>
                    <a href="https://amzn.to/2A9XC5O" target="_blank">
                        NoIR Camera Module
                    </a>
                </li>
                <li>
                    <a href="https://amzn.to/2YdzNCa" target="_blank">
                        A breadboard
                    </a>
                </li>
                <li>
                    <a href="https://amzn.to/2Yee8ts" target="_blank">
                        100 ohm resistor
                    </a>
                </li>
                <li>
                    <a href="https://amzn.to/3dJBSfB" target="_blank">
                        Infrared LEDs
                    </a>
                </li>
                <li> <a href="https://amzn.to/3f1NQSg" target="_blank"> Jumper wires </a> </li>
                <li> A shoe box ðŸ˜€ </li>
            </ul>
        </div>

        <div class="subheading">
            Step 1: Getting a vein image
        </div>

        <div class="blog-section">
            The first step is getting an image to work with - but how can we get an accurate picture of veins?
            Turns out, the hemoglobin in our blood absorbs infrared light (see below). Under the right conditions,
            if we take some infrared LEDs and position them under one's hand, we should be able to get a vein image!
            The naked eye won't see anything since infrared lies
            outside of the visual spectrum, which is why we'll use the RaspberryPi NoIR camera module.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/hemoglobin.png" alt="Hemoglobin Graph">
            <div class="caption">Source: <a href="https://en.wikipedia.org/wiki/Pulse_oximetry">Pule Oximetry</a></div>
        </div>

        <div class="blog-section">
            Now, we need a setup where we can get a consistent image every time. Some variables we
            should control:

            <ul class="list">
                <li>
                    Lighting - we want the IR light to be unobstructed
                </li>
                <li>
                    Height of palm above camera - this depends on the focal length of your camera
                </li>
                <li>
                    Position/orientation of palm in the image
                </li>
            </ul>

            While prototyping, an ordinary shoebox with a palm-sized hole cut out above the camera worked perfectly for
            me.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/box.jpg" alt="Shoebox" id="shoebox-img">
            <div class="caption">Just a regular shoebox</div>
        </div>


        <div class="blog-section">
            When I remade this project using an actual door, I used a simple wooden box with a circle cut out for the
            palm.
        </div>

        <div class="blog-section">
            The circuitry is very simple - we just need to power the IR LEDs. I used 5 LEDs connected in series with a
            100 ohm resistor (you may need a different resistance depending on your LEDs) and a 9V battery. The
            RaspberryPi is on top of the breadboard, with the camera resting on the battery facing up.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/circuit.jpg" alt="Circuit">
            <div class="caption">The circuit</div>
        </div>

        <div class="blog-section blog-section-grey">
            <div class="strong-big">
                Make sure you have a sturdy setup.
            </div>

            I taped down the breadboard and loose parts to the bottom of the
            shoe box, but I suggest using something more secure, like zip ties.
        </div>

        <div class="blog-section">
            Now, let's take an image:
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/palm1.jpg" alt="First Palm Image">
        </div>

        <div class="blog-section">
            Cool! A somewhat distinguishable vein pattern. Let's set up auto-cropping, as we're only concerned with the
            palm. This is the command I used to produce a 600x600 image (you'll want it to be square).
        </div>

        <pre>
            <code class="code-section">
  raspistill -vf -w 600 -h 600 -roi 0.46,0.34,0.25,0.25 -o pic.jpg
            </code>
        </pre>

        <div class="blog-section">
            Those 4 coordinates, <span class="code-inline">(0.46, 0.34, 0.25, 0.25)</span>, define the <a
                target="_blank"
                href="https://picamera.readthedocs.io/en/release-1.10/api_camera.html#picamera.camera.PiCamera.zoom">ROI</a>
            (region of interest). Yours will vary based on
            the positioning of the camera - it takes some trial and error, which is why you don't want your setup to move
            around.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/palm2.png" alt="Post-crop image">
            <div class="caption">Post-crop image</div>
        </div>

        <div class="blog-section">
            Now we have this isolated image of our palm. Next, we need to perform some image processing before we can
            actually make use of it.
        </div>

        <div class="subheading">
            Step 2: Image Processing
        </div>

        <div class="blog-section">
            First, install <a
                href="https://www.pyimagesearch.com/2017/09/04/raspbian-stretch-install-opencv-3-python-on-your-raspberry-pi/"
                target="_blank">OpenCV</a> on your Pi. Then, let's load the image and convert it to
            grayscale.
        </div>

        <pre>
            <code class="code-section">
  # Load the 600x600 image and convert to grayscale
  img = cv2.imread("pic.jpg") 
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            </code>
        </pre>

        <div class="blog-section">
            Let's start by reducing some of the noise. Luckily, OpenCV has a function for that:
        </div>
        <pre>
            <code class="code-section">
  noiseReduced = cv2.fastNlMeansDenoising(gray) 
            </code>
        </pre>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/palm3.png" alt="Post-crop image">
            <div class="caption">After image denoising</div>
        </div>

        <div class="blog-section">
            Much smoother. Now, we need to increase the contrast to really make the veins stand out. The method I used
            was <a href="https://www.math.uci.edu/icamp/courses/math77c/demos/hist_eq.pdf" target="_blank">histogram
                equalization</a>. This distributes the intensities of the pixels in the image, "equalizing" the
            histogram. We then invert the image, since many OpenCV functions assume the background is black and
            foreground is white.
        </div>

        <pre>
            <code class="code-section">
  # Histogram Equalization
  kernel = np.ones((7,7),np.uint8)
  img = cv2.morphologyEx(noiseReduced, cv2.MORPH_OPEN, kernel)
  img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)
  img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])
  img_output = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR)
            </code>
        </pre>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/palm4.png" alt="Post-equalization vein image">
            <div class="caption">After histogram equalization and inversion</div>
        </div>

        <div class="blog-section">
            That made quite a big difference. A lot of the "skin" is gone (now black), with the vein pattern being
            largely white. It's still not quite ready yet - there's a lot of redundant data in this image.
        </div>

        <div class="blog-section">
            <i>Erosion</i> is a technique used to strip away outer layers of data in images. For example, if we want to
            "thin"
            an image of the letter <span class="code-inline">j</span> using erosion:
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/j-comparision.jpeg" alt="Skeletonization example">
            <div class="caption">Source: <a
                    href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/erosion_dilatation/erosion_dilatation.html"
                    target="_blank">Erosion Dilation</a></div>
        </div>

        <div class="blog-section">
            Similarly, we want to do this with the vein image. Let's "skeletonize" it, using repeated erosion.
        </div>

        <pre>
            <code class="code-section">
  img = gray.copy()
  skel = img.copy()
  skel[:,:] = 0
  kernel = cv2.getStructuringElement(cv2.MORPH_CROSS, (5,5))
  while cv2.countNonZero(img) > 0:
      eroded = cv2.morphologyEx(img, cv2.MORPH_ERODE, kernel)
      temp = cv2.morphologyEx(eroded, cv2.MORPH_DILATE, kernel)
      temp  = cv2.subtract(img, temp)
      skel = cv2.bitwise_or(skel, temp)
      img[:,:] = eroded[:,:]
            </code>
        </pre>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/skeleton.png" alt="Skeletonized vein image">
            <div class="caption">After skeletonization (repeated erosion)</div>
        </div>

        <div class="blog-section">
            Now we're talking! I applied a quick threshold to make the veins more visible. Every pixel which is 5 or
            higher
            (everything very dark gray or lighter) will become 255 (white).
        </div>

        <pre>
            <code class="code-section">
  ret, thr = cv2.threshold(skel, 5,255, cv2.THRESH_BINARY);
    </code>
</pre>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/final-palm.png" alt="Final palm image">
            <div class="caption">The final result!</div>
        </div>

        <div class="blog-section">
            To see how accurate this was, I overlayed the vein pattern over the original image to see if there was a
            correlation.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/palm-overlay.jpeg" alt="Overlay">
            <div class="caption">The result overlayed on the original image</div>
        </div>

        <div class="blog-section">
            It's looking good! Not perfect, but it should be more than good enough for our purposes.
        </div>
        <div class="blog-section">
            Now we know how to extract the vein pattern given an image of a palm. The next step is authentication.
        </div>

        <div class="subheading">Step 3: Authentication</div>
        <div class="blog-section">
            To be able to authenticate, you'll need <a href="https://www.tensorflow.org/install/install_raspbian"
                target="_blank">TensorFlow</a> installed. We'll be using a basic classification method.
        </div>
        <div class="blog-section">
            I didn't have access to any public palm-vein data upon completing this project, so my variation was to
            authenticate between multiple hands instead (which I could provide training data for myself). So, instead of
            authenticating between my hand and any other random hand, the model will authenticate between multiple
            pre-trained hands (I used my left and my right). The code is the same, just the training data varies.
        </div>
        <div class="blog-section">
            To start, we need sets of training data. I took 40 total photos for my right and left palms (20 each), and
            processed all of the images to get the vein patterns.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/training-data.jpeg" alt="Training data">
            <div class="caption">Training data</div>
        </div>

        <div class="blog-section">
            Let's throw these images in a model. <span class="code-inline">train_images</span> will be a numpy array of
            images represented as 2d pixel
            arrays. <span class="code-inline">train_labels</span> will be the index of the correct image classification.
            The i-th image in <span class="code-inline">train_images</span>
            will have classification <span class="code-inline">classes[train_labels[i]]</span>.
        </div>

        <div class="blog-section">
            Finally, we normalize the images to be floats from 0 - 1 instead of 0 - 255.
        </div>

        <pre>
            <code class="code-section">
  classes = ["left", "right"]
  num_right_train = 20
  num_left_train = 20
  # 1 for right, 0 for left
  train_labels = np.array([1]*num_right_train + [0]*num_left_train)
  train_images = np.array([])
  for i in range(num_right_train):
     pic = np.array(Image.open("images/right_thr" + str(i) + ".jpg"))
     train_images = np.vstack((train_images, np.array([pic])))
  for i in range(num_left_train):
     pic = np.array(Image.open("images/left_thr" + str(i) + ".jpg"))
     train_images = np.vstack((train_images, np.array([pic])))
  train_images = train_images / 255.0
            </code>
        </pre>

        <div class="blog-section">
            We're now ready to compile and train our model! We need the keras module from TensorFlow for this.
        </div>

        <pre>
            <code class="code-section">
  # Source: https://www.tensorflow.org/tutorials/keras/basic_classification
  model = keras.Sequential([
     keras.layers.Flatten(input_shape=(600, 600)), # dimensions of the image
     keras.layers.Dense(64, activation=tf.nn.relu),
     keras.layers.Dense(2, activation=tf.nn.softmax)
  ])
  model.compile(optimizer=tf.train.AdamOptimizer(), 
         loss='sparse_categorical_crossentropy',
         metrics=['accuracy'])
  model.fit(train_images, train_labels, epochs=5)
            </code>
        </pre>

        <div class="blog-section">
            Training took around 15 - 25 seconds for the 40 images. Now for the moment of truth - let's see if this
            model is any good.
        </div>

        <div class="subheading">Step 4: Testing and Results</div>

        <div class="blog-section">
            I took 30 more photos of my palms (15 left, 15 right) to use as test data.
            I set up the <span class="code-inline">test_images</span> and <span class="code-inline">test_labels</span>
            in the same way as the training data.
        </div>

        <pre>
            <code class="code-section">
  test_loss, test_acc = model.evaluate(test_images, test_labels)
  print('Test accuracy: ', test_acc)
            </code>
        </pre>

        <div class="blog-section">
            I surprisingly managed to get around <strong>93% accuracy</strong> with this model.
            I'm sure it can be significantly improved with a more training data, a higher quality camera, and better IR
            LEDs. Regardless, I was satisfied to even extract the vein data, let alone get a high accuracy!
        </div>

        <div class="subheading">Final Thoughts</div>
        <div class="blog-section">
            This project was inexpensive, unique, and a lot of fun! The remake of this project, using an actual door and locking mechanism,
            won first place at Hack the North 2018.
        </div>

        <div class="img-wrapper">
            <img class="blog-img-content" src="../img/htn.jpg" alt="Hack The North winners">
            <div class="caption">Hack the North 2018</div>
        </div>
        <div class="blog-section">
            I've gotten plenty of emails from others who have recreated this project themselves - get in touch if you do too! ðŸ˜„
            </div>
    </div>

    <div class="signature">
        Ibrahim
    </div>


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/styles/monokai.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.3/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"
        integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
        integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
        crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
        integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
        crossorigin="anonymous"></script>
</body>

</html>